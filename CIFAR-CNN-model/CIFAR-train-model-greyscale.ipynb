{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f8b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8039f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function provided by CIFAR dataset creators to unpackage files\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with keys and values of x\n",
    "    z.update(y)    # modifies z with keys and values of y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fe90f",
   "metadata": {},
   "source": [
    "## Data Loading and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f796f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = unpickle('data\\cifar-10-batches-py\\data_batch_1')\n",
    "for i in range(4):\n",
    "     batch = unpickle(f'data\\cifar-10-batches-py\\data_batch_{i+2}')\n",
    "     train = merge_two_dicts(train, batch)\n",
    "train[b'labels'] = np.asarray(train[b'labels'])\n",
    "\n",
    "test = unpickle('data\\cifar-10-batches-py\\\\test_batch')\n",
    "test[b'labels'] = np.asarray(test[b'labels'])\n",
    "\n",
    "classes = unpickle('data\\cifar-10-batches-py\\\\batches.meta')[b'label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45552da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Change data from (10000, 3072) array to (10000, 32, 32, 1) greyscale images\n",
    "Greyscale is better since the color of the images won't assist in classification.\n",
    "Greyscale is also faster when training and predicting.\n",
    "'''\n",
    "\n",
    "new_data = np.zeros((10000, 32, 32, 3))\n",
    "data = train[b'data']\n",
    "\n",
    "r = np.reshape(data[:, 0:1024], (10000, 32, 32))\n",
    "g = np.reshape(data[:, 1024:2048], (10000, 32, 32))\n",
    "b = np.reshape(data[:, 2048:3072], (10000, 32, 32))\n",
    "new_data[:, :, :, 2] = r\n",
    "new_data[:, :, :, 1] = g\n",
    "new_data[:, :, :, 0] = b\n",
    "\n",
    "grey = np.zeros((10000, 32, 32, 1))\n",
    "for i in range(len(new_data)):\n",
    "    grey[i, :, :, 0] = cv2.cvtColor(new_data[i].astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "# Add reshaped data back to dictionary while dividing by 255 to normalize\n",
    "train[b'data'] = grey/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a174b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test dataset\n",
    "size = len(test[b'labels'])\n",
    "new_data = np.zeros((size, 32, 32, 3))\n",
    "data = test[b'data']\n",
    "\n",
    "r = np.reshape(data[:, 0:1024], (size, 32, 32))\n",
    "g = np.reshape(data[:, 1024:2048], (size, 32, 32))\n",
    "b = np.reshape(data[:, 2048:3072], (size, 32, 32))\n",
    "new_data[:, :, :, 2] = r\n",
    "new_data[:, :, :, 1] = g\n",
    "new_data[:, :, :, 0] = b\n",
    "\n",
    "grey = np.zeros((size, 32, 32, 1))\n",
    "for i in range(len(new_data)):\n",
    "    grey[i, :, :, 0] = cv2.cvtColor(new_data[i].astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "test[b'data'] = grey/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d10122",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30712aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)\n",
    "input_shape = (10000, 32, 32, 1)\n",
    "'''\n",
    "Defining the model below. Uses 4 layers of 2d CNN's with a \n",
    "convolution window of height and width of 3 and 32 filters.\n",
    "Values are good for providing max accuracy while avoiding overfitting \n",
    "and limiting training and prediction time.\n",
    "Padding is set to \"same\" to preserve information at the edges since\n",
    "the dataset includes images with the subject is not centered.\n",
    "'''\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=input_shape[1:]),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6583d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer used since it is the best for image classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bed1ba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f145017e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "141/141 - 6s - loss: 2.1335 - accuracy: 0.1944 - val_loss: 1.8768 - val_accuracy: 0.3150 - 6s/epoch - 44ms/step\n",
      "Epoch 2/25\n",
      "141/141 - 6s - loss: 1.8082 - accuracy: 0.3228 - val_loss: 1.6952 - val_accuracy: 0.3810 - 6s/epoch - 41ms/step\n",
      "Epoch 3/25\n",
      "141/141 - 5s - loss: 1.7046 - accuracy: 0.3646 - val_loss: 1.6179 - val_accuracy: 0.3900 - 5s/epoch - 38ms/step\n",
      "Epoch 4/25\n",
      "141/141 - 5s - loss: 1.6123 - accuracy: 0.4050 - val_loss: 1.7150 - val_accuracy: 0.3560 - 5s/epoch - 35ms/step\n",
      "Epoch 5/25\n",
      "141/141 - 5s - loss: 1.5236 - accuracy: 0.4461 - val_loss: 1.5503 - val_accuracy: 0.4460 - 5s/epoch - 38ms/step\n",
      "Epoch 6/25\n",
      "141/141 - 6s - loss: 1.4514 - accuracy: 0.4672 - val_loss: 1.4514 - val_accuracy: 0.4690 - 6s/epoch - 39ms/step\n",
      "Epoch 7/25\n",
      "141/141 - 5s - loss: 1.3699 - accuracy: 0.5043 - val_loss: 1.3886 - val_accuracy: 0.5170 - 5s/epoch - 37ms/step\n",
      "Epoch 8/25\n",
      "141/141 - 5s - loss: 1.3157 - accuracy: 0.5308 - val_loss: 1.3542 - val_accuracy: 0.5260 - 5s/epoch - 38ms/step\n",
      "Epoch 9/25\n",
      "141/141 - 5s - loss: 1.2571 - accuracy: 0.5544 - val_loss: 1.3296 - val_accuracy: 0.5230 - 5s/epoch - 37ms/step\n",
      "Epoch 10/25\n",
      "141/141 - 6s - loss: 1.2090 - accuracy: 0.5751 - val_loss: 1.3236 - val_accuracy: 0.5270 - 6s/epoch - 41ms/step\n",
      "Epoch 11/25\n",
      "141/141 - 6s - loss: 1.1622 - accuracy: 0.5896 - val_loss: 1.3990 - val_accuracy: 0.5200 - 6s/epoch - 40ms/step\n",
      "Epoch 12/25\n",
      "141/141 - 5s - loss: 1.1192 - accuracy: 0.6057 - val_loss: 1.2878 - val_accuracy: 0.5370 - 5s/epoch - 38ms/step\n",
      "Epoch 13/25\n",
      "141/141 - 5s - loss: 1.0755 - accuracy: 0.6199 - val_loss: 1.2779 - val_accuracy: 0.5520 - 5s/epoch - 38ms/step\n",
      "Epoch 14/25\n",
      "141/141 - 6s - loss: 1.0422 - accuracy: 0.6326 - val_loss: 1.2972 - val_accuracy: 0.5570 - 6s/epoch - 40ms/step\n",
      "Epoch 15/25\n",
      "141/141 - 5s - loss: 1.0049 - accuracy: 0.6480 - val_loss: 1.2676 - val_accuracy: 0.5620 - 5s/epoch - 34ms/step\n",
      "Epoch 16/25\n",
      "141/141 - 5s - loss: 0.9591 - accuracy: 0.6651 - val_loss: 1.2548 - val_accuracy: 0.5680 - 5s/epoch - 33ms/step\n",
      "Epoch 17/25\n",
      "141/141 - 5s - loss: 0.9277 - accuracy: 0.6734 - val_loss: 1.2617 - val_accuracy: 0.5830 - 5s/epoch - 38ms/step\n",
      "Epoch 18/25\n",
      "141/141 - 5s - loss: 0.8905 - accuracy: 0.6881 - val_loss: 1.2919 - val_accuracy: 0.5600 - 5s/epoch - 37ms/step\n",
      "Epoch 19/25\n",
      "141/141 - 5s - loss: 0.8519 - accuracy: 0.7012 - val_loss: 1.2939 - val_accuracy: 0.5650 - 5s/epoch - 37ms/step\n",
      "Epoch 20/25\n",
      "141/141 - 5s - loss: 0.8206 - accuracy: 0.7093 - val_loss: 1.3264 - val_accuracy: 0.5490 - 5s/epoch - 33ms/step\n",
      "Epoch 21/25\n",
      "141/141 - 5s - loss: 0.7888 - accuracy: 0.7252 - val_loss: 1.3103 - val_accuracy: 0.5660 - 5s/epoch - 35ms/step\n",
      "Epoch 22/25\n",
      "141/141 - 6s - loss: 0.7533 - accuracy: 0.7364 - val_loss: 1.3296 - val_accuracy: 0.5620 - 6s/epoch - 42ms/step\n",
      "Epoch 23/25\n",
      "141/141 - 6s - loss: 0.7289 - accuracy: 0.7469 - val_loss: 1.3663 - val_accuracy: 0.5540 - 6s/epoch - 41ms/step\n",
      "Epoch 24/25\n",
      "141/141 - 5s - loss: 0.7069 - accuracy: 0.7504 - val_loss: 1.3608 - val_accuracy: 0.5720 - 5s/epoch - 33ms/step\n",
      "Epoch 25/25\n",
      "141/141 - 5s - loss: 0.6571 - accuracy: 0.7674 - val_loss: 1.3454 - val_accuracy: 0.5770 - 5s/epoch - 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b39d85848>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minibatch gradient descent used with 64 batch size and a 20% holdout size\n",
    "# Epochs set to 25 since that is where validation accuracy peaks\n",
    "batch_size = 64\n",
    "model.fit(train[b'data'], train[b'labels'],\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.2,\n",
    "          epochs=25,\n",
    "          verbose=2\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f276b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 1.3951 - accuracy: 0.5708 - 2s/epoch - 7ms/step\n",
      "\n",
      "Test accuracy: 0.5708000063896179\n"
     ]
    }
   ],
   "source": [
    "# Accuracy should come out to mid to high 50's\n",
    "test_loss, test_acc = model.evaluate(test[b'data'], test[b'labels'], verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e45e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax used since there are more than 2 classes\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy increases using images with clearer subjects\n",
    "\n",
    "#Enter .jpg image from prediction-images folder to get a prediction using model\n",
    "image_name = input('Enter image name: ')\n",
    "\n",
    "image = cv2.imread('prediction-images\\\\'+image_name+'.jpg')\n",
    "image = cv2.resize(image, (32, 32))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = np.array([image])\n",
    "image = image/255.0\n",
    "predictions = probability_model.predict(image)\n",
    "\n",
    "print(classes[np.argmax(predictions[0])].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5d748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b188886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
